# src/app/cli/echo.py

from app.common.messages import Message
from app.common.state import AgentState
from app.graph.simple_graph import build_simple_graph
from app.infra.llm.vllm_client import VllmLLMClient


def main() -> None:
    """
    åŸºäº LangGraph + vLLM(Qwen) çš„å‘½ä»¤è¡Œå¯¹è¯å…¥å£ã€‚

    - ä½¿ç”¨ VllmLLMClient è°ƒç”¨æœ¬åœ° vLLM OpenAI-compatible æœåŠ¡
    """
    print("ğŸ”¹ continual_agent_lab CLI - LangGraph + Qwen(vLLM) æ¨¡å¼")
    print("æç¤ºï¼šè¯·ç¡®ä¿å¦ä¸€ä¸ªç»ˆç«¯é‡Œå·²ç»å¯åŠ¨äº† vLLM æœåŠ¡å™¨ã€‚")
    print("è¾“å…¥å†…å®¹åæŒ‰å›è½¦ï¼Œè¾“å…¥ 'exit' / 'quit' / 'q' é€€å‡ºã€‚\n")

    # 1. åˆ›å»ºä¸€ä¸ª vLLM å®¢æˆ·ç«¯å®ä¾‹
    llm_client = VllmLLMClient()

    # 2. æŠŠå®¢æˆ·ç«¯æ³¨å…¥åˆ°å›¾é‡Œ
    graph = build_simple_graph(llm_client)

    messages: list[Message] = []

    while True:
        user_text = input("ä½ ï¼š").strip()

        if user_text.lower() in ("exit", "quit", "q"):
            print("ğŸ‘‹ å†è§ï¼")
            break

        messages.append(Message(role="user", content=user_text))

        state: AgentState = {"messages": messages}

        # è°ƒç”¨ LangGraph å›¾ï¼ˆå†…éƒ¨ä¼šç”¨ vLLM + Qwen ç”Ÿæˆå›å¤ï¼‰
        final_state = graph.invoke(state)

        messages = final_state["messages"]
        last_msg = messages[-1]

        print(f"agentï¼š{last_msg.content}")


if __name__ == "__main__":
    main()
